{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloaded 104 files.\n",
      "Skipped 0 files due to size limits or patterns.\n",
      "Base path for relative paths: pydantic\n",
      "Include patterns: ['*.md', '*.py']\n",
      "Exclude patterns: None\n",
      "\n",
      "Files in dictionary:\n",
      "  __init__.py\n",
      "  _internal/__init__.py\n",
      "  _internal/_config.py\n",
      "  _internal/_core_metadata.py\n",
      "  _internal/_core_utils.py\n",
      "  _internal/_dataclasses.py\n",
      "  _internal/_decorators.py\n",
      "  _internal/_decorators_v1.py\n",
      "  _internal/_discriminated_union.py\n",
      "  _internal/_docs_extraction.py\n",
      "  _internal/_fields.py\n",
      "  _internal/_forward_ref.py\n",
      "  _internal/_generate_schema.py\n",
      "  _internal/_generics.py\n",
      "  _internal/_git.py\n",
      "  _internal/_import_utils.py\n",
      "  _internal/_internal_dataclass.py\n",
      "  _internal/_known_annotated_metadata.py\n",
      "  _internal/_mock_val_ser.py\n",
      "  _internal/_model_construction.py\n",
      "  _internal/_namespace_utils.py\n",
      "  _internal/_repr.py\n",
      "  _internal/_schema_gather.py\n",
      "  _internal/_schema_generation_shared.py\n",
      "  _internal/_serializers.py\n",
      "  _internal/_signature.py\n",
      "  _internal/_typing_extra.py\n",
      "  _internal/_utils.py\n",
      "  _internal/_validate_call.py\n",
      "  _internal/_validators.py\n",
      "  _migration.py\n",
      "  alias_generators.py\n",
      "  aliases.py\n",
      "  annotated_handlers.py\n",
      "  class_validators.py\n",
      "  color.py\n",
      "  config.py\n",
      "  dataclasses.py\n",
      "  datetime_parse.py\n",
      "  decorator.py\n",
      "  deprecated/__init__.py\n",
      "  deprecated/class_validators.py\n",
      "  deprecated/config.py\n",
      "  deprecated/copy_internals.py\n",
      "  deprecated/decorator.py\n",
      "  deprecated/json.py\n",
      "  deprecated/parse.py\n",
      "  deprecated/tools.py\n",
      "  env_settings.py\n",
      "  error_wrappers.py\n",
      "  errors.py\n",
      "  experimental/__init__.py\n",
      "  experimental/arguments_schema.py\n",
      "  experimental/pipeline.py\n",
      "  fields.py\n",
      "  functional_serializers.py\n",
      "  functional_validators.py\n",
      "  generics.py\n",
      "  json.py\n",
      "  json_schema.py\n",
      "  main.py\n",
      "  mypy.py\n",
      "  networks.py\n",
      "  parse.py\n",
      "  plugin/__init__.py\n",
      "  plugin/_loader.py\n",
      "  plugin/_schema_validator.py\n",
      "  root_model.py\n",
      "  schema.py\n",
      "  tools.py\n",
      "  type_adapter.py\n",
      "  types.py\n",
      "  typing.py\n",
      "  utils.py\n",
      "  v1/__init__.py\n",
      "  v1/_hypothesis_plugin.py\n",
      "  v1/annotated_types.py\n",
      "  v1/class_validators.py\n",
      "  v1/color.py\n",
      "  v1/config.py\n",
      "  v1/dataclasses.py\n",
      "  v1/datetime_parse.py\n",
      "  v1/decorator.py\n",
      "  v1/env_settings.py\n",
      "  v1/error_wrappers.py\n",
      "  v1/errors.py\n",
      "  v1/fields.py\n",
      "  v1/generics.py\n",
      "  v1/json.py\n",
      "  v1/main.py\n",
      "  v1/mypy.py\n",
      "  v1/networks.py\n",
      "  v1/parse.py\n",
      "  v1/schema.py\n",
      "  v1/tools.py\n",
      "  v1/types.py\n",
      "  v1/typing.py\n",
      "  v1/utils.py\n",
      "  v1/validators.py\n",
      "  v1/version.py\n",
      "  validate_call_decorator.py\n",
      "  validators.py\n",
      "  version.py\n",
      "  warnings.py\n",
      "\n",
      "Sample file: functional_serializers.py\n",
      "Content preview: \"\"\"This module contains related classes and functions for serialization.\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "import dataclasses\n",
      "from functools import partial, partialmethod\n",
      "from typing import TYPE...\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gh_crawler import gh_crawler\n",
    "import os \n",
    "\n",
    "import github_crawler\n",
    "\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "# print(f\"Token found: {github_token is not None}\")  # ← Add this debug line\n",
    "# print(f\"Token value: {github_token[:5]}...\" if github_token else \"None\")  # ← Check first 10 chars\n",
    "\n",
    "if not github_token:\n",
    "    print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n",
    "            \"Private repositories will not be accessible without a token.\\n\"\n",
    "            \"To access private repos, set the environment variable or pass the token explicitly.\")\n",
    "\n",
    "repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n",
    "\n",
    "# Example: Get Python and Markdown files, but exclude test files\n",
    "files, stats = github_crawler.crawl_github_files_py(\n",
    "    repo_url, \n",
    "    token=github_token,\n",
    "    max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n",
    "    use_relative_paths=True,  # Enable relative paths\n",
    "    include_patterns=[\"*.py\", \"*.md\"],  # Include Python and Markdown files\n",
    ")\n",
    "\n",
    "print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n",
    "print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n",
    "print(f\"Base path for relative paths: {stats['base_path']}\")\n",
    "print(f\"Include patterns: {stats['include_patterns']}\")\n",
    "print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n",
    "\n",
    "print(\"\\nFiles in dictionary:\")\n",
    "for file_path in sorted(files.keys()):\n",
    "    print(f\"  {file_path}\")\n",
    "\n",
    "if files:\n",
    "    sample_file = next(iter(files))\n",
    "    print(f\"\\nSample file: {sample_file}\")\n",
    "    print(f\"Content preview: {files[sample_file][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: __init__.py (15395 bytes) \n",
      "Downloaded: _internal/__init__.py (0 bytes) \n",
      "Downloaded: _internal/_config.py (14253 bytes) \n",
      "Downloaded: _internal/_core_metadata.py (5162 bytes) \n",
      "Downloaded: _internal/_core_utils.py (6746 bytes) \n",
      "Downloaded: _internal/_dataclasses.py (8909 bytes) \n",
      "Downloaded: _internal/_decorators.py (32638 bytes) \n",
      "Downloaded: _internal/_decorators_v1.py (6185 bytes) \n",
      "Downloaded: _internal/_discriminated_union.py (25478 bytes) \n",
      "Downloaded: _internal/_docs_extraction.py (3831 bytes) \n",
      "Downloaded: _internal/_fields.py (20647 bytes) \n",
      "Downloaded: _internal/_forward_ref.py (611 bytes) \n",
      "Downloaded: _internal/_generate_schema.py (131620 bytes) \n",
      "Downloaded: _internal/_generics.py (23849 bytes) \n",
      "Downloaded: _internal/_git.py (804 bytes) \n",
      "Downloaded: _internal/_import_utils.py (402 bytes) \n",
      "Downloaded: _internal/_internal_dataclass.py (144 bytes) \n",
      "Downloaded: _internal/_known_annotated_metadata.py (16426 bytes) \n",
      "Downloaded: _internal/_mock_val_ser.py (8885 bytes) \n",
      "Downloaded: _internal/_model_construction.py (35228 bytes) \n",
      "Downloaded: _internal/_namespace_utils.py (12878 bytes) \n",
      "Downloaded: _internal/_repr.py (5081 bytes) \n",
      "Downloaded: _internal/_schema_gather.py (8863 bytes) \n",
      "Downloaded: _internal/_schema_generation_shared.py (4842 bytes) \n",
      "Downloaded: _internal/_serializers.py (1474 bytes) \n",
      "Downloaded: _internal/_signature.py (6779 bytes) \n",
      "Downloaded: _internal/_typing_extra.py (28216 bytes) \n",
      "Downloaded: _internal/_utils.py (15248 bytes) \n",
      "Downloaded: _internal/_validate_call.py (5321 bytes) \n",
      "Downloaded: _internal/_validators.py (20610 bytes) \n",
      "Downloaded: _migration.py (11907 bytes) \n",
      "Downloaded: alias_generators.py (2124 bytes) \n",
      "Downloaded: aliases.py (4937 bytes) \n",
      "Downloaded: annotated_handlers.py (4407 bytes) \n",
      "Downloaded: class_validators.py (148 bytes) \n",
      "Downloaded: color.py (21481 bytes) \n",
      "Downloaded: config.py (42048 bytes) \n",
      "Downloaded: dataclasses.py (16215 bytes) \n",
      "Downloaded: datetime_parse.py (150 bytes) \n",
      "Downloaded: decorator.py (145 bytes) \n",
      "Downloaded: deprecated/__init__.py (0 bytes) \n",
      "Downloaded: deprecated/class_validators.py (10245 bytes) \n",
      "Downloaded: deprecated/config.py (2663 bytes) \n",
      "Downloaded: deprecated/copy_internals.py (7616 bytes) \n",
      "Downloaded: deprecated/decorator.py (10845 bytes) \n",
      "Downloaded: deprecated/json.py (4657 bytes) \n",
      "Downloaded: deprecated/parse.py (2511 bytes) \n",
      "Downloaded: deprecated/tools.py (3330 bytes) \n",
      "Downloaded: env_settings.py (148 bytes) \n",
      "Downloaded: error_wrappers.py (150 bytes) \n",
      "Downloaded: errors.py (6034 bytes) \n",
      "Downloaded: experimental/__init__.py (328 bytes) \n",
      "Downloaded: experimental/arguments_schema.py (1866 bytes) \n",
      "Downloaded: experimental/pipeline.py (23910 bytes) \n",
      "Downloaded: fields.py (63787 bytes) \n",
      "Downloaded: functional_serializers.py (17102 bytes) \n",
      "Downloaded: functional_validators.py (29560 bytes) \n",
      "Downloaded: generics.py (144 bytes) \n",
      "Downloaded: json.py (140 bytes) \n",
      "Downloaded: json_schema.py (115430 bytes) \n",
      "Downloaded: main.py (80829 bytes) \n",
      "Downloaded: mypy.py (58995 bytes) \n",
      "Downloaded: networks.py (41446 bytes) \n",
      "Downloaded: parse.py (141 bytes) \n",
      "Downloaded: plugin/__init__.py (6965 bytes) \n",
      "Downloaded: plugin/_loader.py (2167 bytes) \n",
      "Downloaded: plugin/_schema_validator.py (5267 bytes) \n",
      "Skipping py.typed: Does not match include/exclude patterns\n",
      "Downloaded: root_model.py (6279 bytes) \n",
      "Downloaded: schema.py (142 bytes) \n",
      "Downloaded: tools.py (141 bytes) \n",
      "Downloaded: type_adapter.py (31171 bytes) \n",
      "Downloaded: types.py (104781 bytes) \n",
      "Downloaded: typing.py (138 bytes) \n",
      "Downloaded: utils.py (141 bytes) \n",
      "Downloaded: v1/__init__.py (2946 bytes) \n",
      "Downloaded: v1/_hypothesis_plugin.py (14847 bytes) \n",
      "Downloaded: v1/annotated_types.py (3157 bytes) \n",
      "Downloaded: v1/class_validators.py (14672 bytes) \n",
      "Downloaded: v1/color.py (16844 bytes) \n",
      "Downloaded: v1/config.py (6532 bytes) \n",
      "Downloaded: v1/dataclasses.py (18172 bytes) \n",
      "Downloaded: v1/datetime_parse.py (7724 bytes) \n",
      "Downloaded: v1/decorator.py (10339 bytes) \n",
      "Downloaded: v1/env_settings.py (14105 bytes) \n",
      "Downloaded: v1/error_wrappers.py (5196 bytes) \n",
      "Downloaded: v1/errors.py (17726 bytes) \n",
      "Downloaded: v1/fields.py (50649 bytes) \n",
      "Downloaded: v1/generics.py (17871 bytes) \n",
      "Downloaded: v1/json.py (3390 bytes) \n",
      "Downloaded: v1/main.py (44555 bytes) \n",
      "Downloaded: v1/mypy.py (38949 bytes) \n",
      "Downloaded: v1/networks.py (22124 bytes) \n",
      "Downloaded: v1/parse.py (1821 bytes) \n",
      "Skipping v1/py.typed: Does not match include/exclude patterns\n",
      "Downloaded: v1/schema.py (47801 bytes) \n",
      "Downloaded: v1/tools.py (2881 bytes) \n",
      "Downloaded: v1/types.py (35455 bytes) \n",
      "Downloaded: v1/typing.py (19387 bytes) \n",
      "Downloaded: v1/utils.py (25941 bytes) \n",
      "Downloaded: v1/validators.py (22187 bytes) \n",
      "Downloaded: v1/version.py (1039 bytes) \n",
      "Downloaded: validate_call_decorator.py (4389 bytes) \n",
      "Downloaded: validators.py (146 bytes) \n",
      "Downloaded: version.py (2827 bytes) \n",
      "Downloaded: warnings.py (3772 bytes) \n",
      "\n",
      "Downloaded 104 files.\n",
      "Skipped 0 files due to size limits or patterns.\n",
      "Base path for relative paths: pydantic\n",
      "Include patterns: {'*.md', '*.py'}\n",
      "Exclude patterns: None\n",
      "\n",
      "Files in dictionary:\n",
      "  __init__.py\n",
      "  _internal/__init__.py\n",
      "  _internal/_config.py\n",
      "  _internal/_core_metadata.py\n",
      "  _internal/_core_utils.py\n",
      "  _internal/_dataclasses.py\n",
      "  _internal/_decorators.py\n",
      "  _internal/_decorators_v1.py\n",
      "  _internal/_discriminated_union.py\n",
      "  _internal/_docs_extraction.py\n",
      "  _internal/_fields.py\n",
      "  _internal/_forward_ref.py\n",
      "  _internal/_generate_schema.py\n",
      "  _internal/_generics.py\n",
      "  _internal/_git.py\n",
      "  _internal/_import_utils.py\n",
      "  _internal/_internal_dataclass.py\n",
      "  _internal/_known_annotated_metadata.py\n",
      "  _internal/_mock_val_ser.py\n",
      "  _internal/_model_construction.py\n",
      "  _internal/_namespace_utils.py\n",
      "  _internal/_repr.py\n",
      "  _internal/_schema_gather.py\n",
      "  _internal/_schema_generation_shared.py\n",
      "  _internal/_serializers.py\n",
      "  _internal/_signature.py\n",
      "  _internal/_typing_extra.py\n",
      "  _internal/_utils.py\n",
      "  _internal/_validate_call.py\n",
      "  _internal/_validators.py\n",
      "  _migration.py\n",
      "  alias_generators.py\n",
      "  aliases.py\n",
      "  annotated_handlers.py\n",
      "  class_validators.py\n",
      "  color.py\n",
      "  config.py\n",
      "  dataclasses.py\n",
      "  datetime_parse.py\n",
      "  decorator.py\n",
      "  deprecated/__init__.py\n",
      "  deprecated/class_validators.py\n",
      "  deprecated/config.py\n",
      "  deprecated/copy_internals.py\n",
      "  deprecated/decorator.py\n",
      "  deprecated/json.py\n",
      "  deprecated/parse.py\n",
      "  deprecated/tools.py\n",
      "  env_settings.py\n",
      "  error_wrappers.py\n",
      "  errors.py\n",
      "  experimental/__init__.py\n",
      "  experimental/arguments_schema.py\n",
      "  experimental/pipeline.py\n",
      "  fields.py\n",
      "  functional_serializers.py\n",
      "  functional_validators.py\n",
      "  generics.py\n",
      "  json.py\n",
      "  json_schema.py\n",
      "  main.py\n",
      "  mypy.py\n",
      "  networks.py\n",
      "  parse.py\n",
      "  plugin/__init__.py\n",
      "  plugin/_loader.py\n",
      "  plugin/_schema_validator.py\n",
      "  root_model.py\n",
      "  schema.py\n",
      "  tools.py\n",
      "  type_adapter.py\n",
      "  types.py\n",
      "  typing.py\n",
      "  utils.py\n",
      "  v1/__init__.py\n",
      "  v1/_hypothesis_plugin.py\n",
      "  v1/annotated_types.py\n",
      "  v1/class_validators.py\n",
      "  v1/color.py\n",
      "  v1/config.py\n",
      "  v1/dataclasses.py\n",
      "  v1/datetime_parse.py\n",
      "  v1/decorator.py\n",
      "  v1/env_settings.py\n",
      "  v1/error_wrappers.py\n",
      "  v1/errors.py\n",
      "  v1/fields.py\n",
      "  v1/generics.py\n",
      "  v1/json.py\n",
      "  v1/main.py\n",
      "  v1/mypy.py\n",
      "  v1/networks.py\n",
      "  v1/parse.py\n",
      "  v1/schema.py\n",
      "  v1/tools.py\n",
      "  v1/types.py\n",
      "  v1/typing.py\n",
      "  v1/utils.py\n",
      "  v1/validators.py\n",
      "  v1/version.py\n",
      "  validate_call_decorator.py\n",
      "  validators.py\n",
      "  version.py\n",
      "  warnings.py\n",
      "\n",
      "Sample file: __init__.py\n",
      "Content preview: import typing\n",
      "from importlib import import_module\n",
      "from warnings import warn\n",
      "\n",
      "from ._migration import getattr_migration\n",
      "from .version import VERSION\n",
      "\n",
      "if typing.TYPE_CHECKING:\n",
      "    # import of virtually ...\n",
      "CPU times: total: 2.34 s\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import requests\n",
    "import base64\n",
    "import os\n",
    "import tempfile\n",
    "import git\n",
    "import time\n",
    "import fnmatch\n",
    "from typing import Union, Set, List, Dict, Tuple, Any\n",
    "from urllib.parse import urlparse\n",
    "from dotenv import load_dotenv   # NEW\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "def crawl_github_files(\n",
    "    repo_url, \n",
    "    token=None, \n",
    "    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n",
    "    use_relative_paths: bool = False,\n",
    "    include_patterns: Union[str, Set[str]] = None,\n",
    "    exclude_patterns: Union[str, Set[str]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Crawl files from a specific path in a GitHub repository at a specific commit.\n",
    "\n",
    "    Args:\n",
    "        repo_url (str): URL of the GitHub repository with specific path and commit\n",
    "                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n",
    "        token (str, optional): **GitHub personal access token.**\n",
    "            - **Required for private repositories.**\n",
    "            - **Recommended for public repos to avoid rate limits.**\n",
    "            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n",
    "        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n",
    "        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n",
    "        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n",
    "                                                       If None, all files are included.\n",
    "        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n",
    "                                                       If None, no files are excluded.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with files and statistics\n",
    "    \"\"\"\n",
    "    # Convert single pattern to set\n",
    "    if include_patterns and isinstance(include_patterns, str):\n",
    "        include_patterns = {include_patterns}\n",
    "    if exclude_patterns and isinstance(exclude_patterns, str):\n",
    "        exclude_patterns = {exclude_patterns}\n",
    "\n",
    "    def should_include_file(file_path: str, file_name: str) -> bool:\n",
    "        \"\"\"Determine if a file should be included based on patterns\"\"\"\n",
    "        # If no include patterns are specified, include all files\n",
    "        if not include_patterns:\n",
    "            include_file = True\n",
    "        else:\n",
    "            # Check if file matches any include pattern\n",
    "            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n",
    "\n",
    "        # If exclude patterns are specified, check if file should be excluded\n",
    "        if exclude_patterns and include_file:\n",
    "            # Exclude if file matches any exclude pattern\n",
    "            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n",
    "            return not exclude_file\n",
    "\n",
    "        return include_file\n",
    "\n",
    "    # Detect SSH URL (git@ or .git suffix)\n",
    "    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n",
    "\n",
    "    if is_ssh_url:\n",
    "        # Clone repo via SSH to temp dir\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n",
    "            try:\n",
    "                repo = git.Repo.clone_from(repo_url, tmpdirname)\n",
    "            except Exception as e:\n",
    "                print(f\"Error cloning repo: {e}\")\n",
    "                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n",
    "\n",
    "            # Attempt to checkout specific commit/branch if in URL\n",
    "            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n",
    "            # So rely on default branch, or user can checkout manually later\n",
    "            # Optionally, user can pass ref explicitly in future API\n",
    "\n",
    "            # Walk directory\n",
    "            files = {}\n",
    "            skipped_files = []\n",
    "\n",
    "            for root, dirs, filenames in os.walk(tmpdirname):\n",
    "                for filename in filenames:\n",
    "                    abs_path = os.path.join(root, filename)\n",
    "                    rel_path = os.path.relpath(abs_path, tmpdirname)\n",
    "\n",
    "                    # Check file size\n",
    "                    try:\n",
    "                        file_size = os.path.getsize(abs_path)\n",
    "                    except OSError:\n",
    "                        continue\n",
    "\n",
    "                    if file_size > max_file_size:\n",
    "                        skipped_files.append((rel_path, file_size))\n",
    "                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n",
    "                        continue\n",
    "\n",
    "                    # Check include/exclude patterns\n",
    "                    if not should_include_file(rel_path, filename):\n",
    "                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n",
    "                        continue\n",
    "\n",
    "                    # Read content\n",
    "                    try:\n",
    "                        with open(abs_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "                            content = f.read()\n",
    "                        files[rel_path] = content\n",
    "                        print(f\"Added {rel_path} ({file_size} bytes)\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to read {rel_path}: {e}\")\n",
    "\n",
    "            return {\n",
    "                \"files\": files,\n",
    "                \"stats\": {\n",
    "                    \"downloaded_count\": len(files),\n",
    "                    \"skipped_count\": len(skipped_files),\n",
    "                    \"skipped_files\": skipped_files,\n",
    "                    \"base_path\": None,\n",
    "                    \"include_patterns\": include_patterns,\n",
    "                    \"exclude_patterns\": exclude_patterns,\n",
    "                    \"source\": \"ssh_clone\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n",
    "    parsed_url = urlparse(repo_url)\n",
    "    path_parts = parsed_url.path.strip('/').split('/')\n",
    "    \n",
    "    if len(path_parts) < 2:\n",
    "        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n",
    "    \n",
    "    # Extract the basic components\n",
    "    owner = path_parts[0]\n",
    "    repo = path_parts[1]\n",
    "    \n",
    "    # Setup for GitHub API\n",
    "    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
    "    if token:\n",
    "        headers[\"Authorization\"] = f\"token {token}\"\n",
    "\n",
    "    def fetch_branches(owner: str, repo: str):\n",
    "        \"\"\"Get brancshes of the repository\"\"\"\n",
    "\n",
    "        url = f\"https://api.github.com/repos/{owner}/{repo}/branches\"\n",
    "        response = requests.get(url, headers=headers, timeout=(30, 30))\n",
    "\n",
    "        if response.status_code == 404:\n",
    "            if not token:\n",
    "                print(f\"Error 404: Repository not found or is private.\\n\"\n",
    "                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n",
    "            else:\n",
    "                print(f\"Error 404: Repository not found or insufficient permissions with the provided token.\\n\"\n",
    "                      f\"Please verify the repository exists and the token has access to this repository.\")\n",
    "            return []\n",
    "            \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}\")\n",
    "            return []\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    def check_tree(owner: str, repo: str, tree: str):\n",
    "        \"\"\"Check the repository has the given tree\"\"\"\n",
    "\n",
    "        url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/{tree}\"\n",
    "        response = requests.get(url, headers=headers, timeout=(30, 30))\n",
    "\n",
    "        return True if response.status_code == 200 else False \n",
    "\n",
    "    # Check if URL contains a specific branch/commit\n",
    "    if len(path_parts) > 2 and 'tree' == path_parts[2]:\n",
    "        join_parts = lambda i: '/'.join(path_parts[i:])\n",
    "\n",
    "        branches = fetch_branches(owner, repo)\n",
    "        branch_names = map(lambda branch: branch.get(\"name\"), branches)\n",
    "\n",
    "        # Fetching branches is not successfully\n",
    "        if len(branches) == 0:\n",
    "            return\n",
    "\n",
    "        # To check branch name\n",
    "        relevant_path = join_parts(3)\n",
    "\n",
    "        # Find a match with relevant path and get the branch name\n",
    "        filter_gen = (name for name in branch_names if relevant_path.startswith(name))\n",
    "        ref = next(filter_gen, None)\n",
    "\n",
    "        # If match is not found, check for is it a tree\n",
    "        if ref == None:\n",
    "            tree = path_parts[3]\n",
    "            ref = tree if check_tree(owner, repo, tree) else None\n",
    "\n",
    "        # If it is neither a tree nor a branch name\n",
    "        if ref == None:\n",
    "            print(f\"The given path does not match with any branch and any tree in the repository.\\n\"\n",
    "                  f\"Please verify the path is exists.\")\n",
    "            return\n",
    "\n",
    "        # Combine all parts after the ref as the path\n",
    "        part_index = 5 if '/' in ref else 4\n",
    "        specific_path = join_parts(part_index) if part_index < len(path_parts) else \"\"\n",
    "    else:\n",
    "        # Dont put the ref param to quiery\n",
    "        # and let Github decide default branch\n",
    "        ref = None\n",
    "        specific_path = \"\"\n",
    "    \n",
    "    # Dictionary to store path -> content mapping\n",
    "    files = {}\n",
    "    skipped_files = []\n",
    "    \n",
    "    def fetch_contents(path):\n",
    "        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n",
    "        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
    "        params = {\"ref\": ref} if ref != None else {}\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params, timeout=(30, 30))\n",
    "        \n",
    "        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n",
    "            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "            wait_time = max(reset_time - time.time(), 0) + 1\n",
    "            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            return fetch_contents(path)\n",
    "            \n",
    "        if response.status_code == 404:\n",
    "            if not token:\n",
    "                print(f\"Error 404: Repository not found or is private.\\n\"\n",
    "                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n",
    "            elif not path and ref == 'main':\n",
    "                print(f\"Error 404: Repository not found. Check if the default branch is not 'main'\\n\"\n",
    "                      f\"Try adding branch name to the request i.e. python main.py --repo https://github.com/username/repo/tree/master\")\n",
    "            else:\n",
    "                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n",
    "                      f\"Please verify the token has access to this repository and the path exists.\")\n",
    "            return\n",
    "            \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n",
    "            return\n",
    "        \n",
    "        contents = response.json()\n",
    "        \n",
    "        # Handle both single file and directory responses\n",
    "        if not isinstance(contents, list):\n",
    "            contents = [contents]\n",
    "        \n",
    "        for item in contents:\n",
    "            item_path = item[\"path\"]\n",
    "            \n",
    "            # Calculate relative path if requested\n",
    "            if use_relative_paths and specific_path:\n",
    "                # Make sure the path is relative to the specified subdirectory\n",
    "                if item_path.startswith(specific_path):\n",
    "                    rel_path = item_path[len(specific_path):].lstrip('/')\n",
    "                else:\n",
    "                    rel_path = item_path\n",
    "            else:\n",
    "                rel_path = item_path\n",
    "            \n",
    "            if item[\"type\"] == \"file\":\n",
    "                # Check if file should be included based on patterns\n",
    "                if not should_include_file(rel_path, item[\"name\"]):\n",
    "                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n",
    "                    continue\n",
    "                \n",
    "                # Check file size if available\n",
    "                file_size = item.get(\"size\", 0)\n",
    "                if file_size > max_file_size:\n",
    "                    skipped_files.append((item_path, file_size))\n",
    "                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n",
    "                    continue\n",
    "                \n",
    "                # For files, get raw content\n",
    "                if \"download_url\" in item and item[\"download_url\"]:\n",
    "                    file_url = item[\"download_url\"]\n",
    "                    file_response = requests.get(file_url, headers=headers, timeout=(30, 30))\n",
    "                    \n",
    "                    # Final size check in case content-length header is available but differs from metadata\n",
    "                    content_length = int(file_response.headers.get('content-length', 0))\n",
    "                    if content_length > max_file_size:\n",
    "                        skipped_files.append((item_path, content_length))\n",
    "                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n",
    "                        continue\n",
    "                        \n",
    "                    if file_response.status_code == 200:\n",
    "                        files[rel_path] = file_response.text\n",
    "                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n",
    "                    else:\n",
    "                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n",
    "                else:\n",
    "                    # Alternative method if download_url is not available\n",
    "                    content_response = requests.get(item[\"url\"], headers=headers, timeout=(30, 30))\n",
    "                    if content_response.status_code == 200:\n",
    "                        content_data = content_response.json()\n",
    "                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n",
    "                            # Check size of base64 content before decoding\n",
    "                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n",
    "                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n",
    "                                skipped_files.append((item_path, estimated_size))\n",
    "                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n",
    "                                continue\n",
    "                                \n",
    "                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n",
    "                            files[rel_path] = file_content\n",
    "                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n",
    "                        else:\n",
    "                            print(f\"Unexpected content format for {rel_path}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n",
    "            \n",
    "            elif item[\"type\"] == \"dir\":\n",
    "                # OLD IMPLEMENTATION (comment this block to test new implementation)\n",
    "                # Always recurse into directories without checking exclusions first\n",
    "                # fetch_contents(item_path)\n",
    "\n",
    "                # NEW IMPLEMENTATION (uncomment this block to test optimized version)\n",
    "                # # Check if directory should be excluded before recursing\n",
    "                if exclude_patterns:\n",
    "                    dir_excluded = any(fnmatch.fnmatch(item_path, pattern) or\n",
    "                                    fnmatch.fnmatch(rel_path, pattern) for pattern in exclude_patterns)\n",
    "                    if dir_excluded:\n",
    "                        continue\n",
    "                \n",
    "                # # Only recurse if directory is not excluded\n",
    "                fetch_contents(item_path)\n",
    "    \n",
    "    # Start crawling from the specified path\n",
    "    fetch_contents(specific_path)\n",
    "    \n",
    "    return {\n",
    "        \"files\": files,\n",
    "        \"stats\": {\n",
    "            \"downloaded_count\": len(files),\n",
    "            \"skipped_count\": len(skipped_files),\n",
    "            \"skipped_files\": skipped_files,\n",
    "            \"base_path\": specific_path if use_relative_paths else None,\n",
    "            \"include_patterns\": include_patterns,\n",
    "            \"exclude_patterns\": exclude_patterns\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Get token from environment variable (recommended for private repos)\n",
    "    github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "    if not github_token:\n",
    "        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n",
    "              \"Private repositories will not be accessible without a token.\\n\"\n",
    "              \"To access private repos, set the environment variable or pass the token explicitly.\")\n",
    "    \n",
    "    repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n",
    "    \n",
    "    # Example: Get Python and Markdown files, but exclude test files\n",
    "    result = crawl_github_files(\n",
    "        repo_url, \n",
    "        token=github_token,\n",
    "        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n",
    "        use_relative_paths=True,  # Enable relative paths\n",
    "        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n",
    "    )\n",
    "    \n",
    "    files = result[\"files\"]\n",
    "    stats = result[\"stats\"]\n",
    "    \n",
    "    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n",
    "    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n",
    "    print(f\"Base path for relative paths: {stats['base_path']}\")\n",
    "    print(f\"Include patterns: {stats['include_patterns']}\")\n",
    "    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n",
    "    \n",
    "    # Display all file paths in the dictionary\n",
    "    print(\"\\nFiles in dictionary:\")\n",
    "    for file_path in sorted(files.keys()):\n",
    "        print(f\"  {file_path}\")\n",
    "    \n",
    "    # Example: accessing content of a specific file\n",
    "    if files:\n",
    "        sample_file = next(iter(files))\n",
    "        print(f\"\\nSample file: {sample_file}\")\n",
    "        print(f\"Content preview: {files[sample_file][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90317be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
